```python
from selenium import webdriver
```


```python
driver=webdriver.Chrome("chromedriver.exe")
```


```python
driver.get("https://www.naver.com") #url 접속
```


```python
html=driver.page_source
```


```python
#html
```


```python
#멜론
```


```python
driver.get("http://www.melon.com/chart/index.htm")
```


```python
html=driver.page_source
html
```


```python
from bs4 import BeautifulSoup
```


```python
soup=BeautifulSoup(html, 'html.parser')
```


```python
len(soup.select("tr"))
```


```python
songs=soup.select("tr")
songs[100]
```


```python
songs=soup.select("tr")[1:] #타이틀 제외하고 100곡 저장
```


```python
songs[0]
```


```python
#SyntaxError: invalid syntax
#오타 확인
#크롬드라이버
```


```python
songs[0]
```


```python
song=songs[0]
title=song.select("a")
len(title)
title
```


```python
len(song.select('span'))
song.select('span > a')[0].string
```


```python
len(song.select("div"))
song.select("div.ellipsis.rank01 > span > a")[0].text
```


```python
song.select('div.ellipsis.rank02 > a')[0].text
```


```python
#songs
#노래제목, 가수명 - 상위 30위까지 출력
```


```python
# 바라만 본다 - MSG워너비(M.O.M)
# Next Level - aespa
# ...

for song in songs[:30]:
    print(song.select('span > a')[0].string, '-', song.select('div.ellipsis.rank02 > a')[0].string)

```


```python
###멜론 클롤링 -> 엑셀 저장 -> pandas
```


```python
from selenium import webdriver
from bs4 import BeautifulSoup

driver = webdriver.Chrome('chromedriver.exe')
url = 'http://www.melon.com/chart/index.htm'
driver.get(url)     

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')  
```


```python
song_data = []
rank = 1

songs = soup.select('table > tbody > tr')
for song in songs:                                        
    title = song.select('div.rank01 > span > a')[0].text
    singer = song.select('div.rank02 > a')[0].text
    song_data.append(['Melon', rank, title, singer])
    rank = rank + 1
```


```python
song_data #리스트의 리스트
import pandas as pd
```


```python
song_data
```


```python
data=pd.DataFrame(song_data, columns=['멜론', '순위', '제목', '가수'])
data.head()
```


```python
data.to_excel("melon.xlsx", index=False)
```


```python

```


```python
#유튜브 데이터 추출
```


```python
browser=webdriver.Chrome("chromedriver.exe")
```


```python
url="https://youtube-rank.com/board/bbs/board.php?bo_table=youtube"
```


```python
browser.get(url)
```


```python
html=browser.page_source
soup=BeautifulSoup(html, 'html.parser')
```


```python
soup
```


```python
channelList=soup.select("tr")
```


```python
len(channelList)
```


```python
channelList=channelList[1:101] #제목, 사이트 정보 제거하고 
#순수 랭킹 데이터만 추출하여 저장
```


```python
channel=channelList[0]
```


```python
len(channelList)
```


```python
channel
```


```python
channel
```


```python
category=channel.select("p.category")[0].text.strip()
```


```python
title=channel.select("h1 > a")[0].text.strip()
```


```python
subscriber=channel.select(".subscriber_cnt")[0].text
view=channel.select(".view_cnt")[0].text
video=channel.select(".video_cnt")[0].text
# <td class="subscriber_cnt">6320만</td>
# <td class="view_cnt">186억2604만</td>
# <td class="video_cnt">364개</td>
```


```python
for ch in channelList:
    category=ch.select("p.category")[0].text.strip()
    title=ch.select("h1 > a")[0].text.strip()
    subscriber=ch.select(".subscriber_cnt")[0].text
    view=ch.select(".view_cnt")[0].text
    video=ch.select(".video_cnt")[0].text
    print(title, category, subscriber, view,video)
```


```python
page=1
url="https://youtube-rank.com/board/bbs/board.php?bo_table=youtube&page={}".format(page)
url
```


```python
import time
```


```python
data=[]
for page in range(1,11):
    url="https://youtube-rank.com/board/bbs/board.php?bo_table=youtube&page={}".format(page)
    #print(url)
    browser.get(url) #서버 접속하여 페이지를 가져옴(2초 이내에)
    time.sleep(2)
    #여러 페이지를 읽을때 주의, 인위적으로 delay
    html=browser.page_source #가져온 페이지의 소스코드를 저장
    soup=BeautifulSoup(html, 'html.parser')
    channelList=soup.select("tr")
    channelList=channelList[1:101] 
    for ch in channelList:
        category=ch.select("p.category")[0].text.strip()
        title=ch.select("h1 > a")[0].text.strip()
        subscriber=ch.select(".subscriber_cnt")[0].text
        view=ch.select(".view_cnt")[0].text
        video=ch.select(".video_cnt")[0].text
        data.append([title, category, subscriber, view, video])
        #print(title, category, subscriber, view,video)
        
    #print("-"*100)    
```


```python
data[999]
```


```python
df=pd.DataFrame(data)
df.columns=['title','category','subscriber','view','video']
```


```python
df
df.to_excel("youtube_rank.xlsx", index=False)
```


```python
#sns data 추출
#인스타그램
```


```python
from selenium import webdriver
```


```python
driver=webdriver.Chrome("chromedriver.exe")
```


```python
driver.get("https://www.instagram.com")
time.sleep(2)
```


```python
email="아이디"
inputId=driver.find_elements_by_css_selector("#loginForm > div > div > div > label > input")[0]
inputId.clear()
inputId.send_keys(email)
```


```python
password="비밀번호"
inputPw=driver.find_elements_by_css_selector("#loginForm > div > div > div > label > input")[1]
inputPw.clear()                               
inputPw.send_keys(password)
inputPw.submit()
time.sleep(2)
```

# 제목


```python
# https://www.instagram.com/explore/tags/%EC%A0%9C%EC%A3%BC%EB%8F%84%EB%A7%9B%EC%A7%91/
# https://www.instagram.com/explore/tags/%EC%82%BC%EC%84%B1%EB%8F%99%EB%A7%9B%EC%A7%91/
        
```


```python
word="울릉도맛집"
url="https://www.instagram.com/explore/tags/"+ word
driver.get(url)
```


```python
first=driver.find_element_by_css_selector("div._9AhH0")
first.click()
time.sleep(2)
#react-root > section > main > article > div.EZdmt > div > div > div > div > a > div.eLAPa > div._9AhH0
```


```python
#게시물 텍스트 데이터 스크래핑
```


```python

right=driver.find_element_by_css_selector("a._65Bje.coreSpriteRightPaginationArrow")
right.click()
time.sleep(2)

```


```python
# left=driver.find_element_by_css_selector("a.ITLxV.coreSpriteLeftPaginationArrow ")
# left.click()
# time.sleep(2)
```


```python

```


```python
#인스타그램 접속(아이디/비밀번호) -> 검색어 : 칸쿤 맛집 -> 

# 첫번째 검색 페이지에서 좋아요, 작성일 추출 -> 출력
#adv.  5개 페이지에 대해서 동일한 작업 수행
                                                    

```


```python

```


```python

```


```python

```
